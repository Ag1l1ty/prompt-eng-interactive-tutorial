{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Capitulo 1: Estructura Basica de Prompts\n\n**Nota:** Los prompts de ejemplo se mantienen en ingles ya que las tecnicas de ingenieria de prompts son independientes del idioma y Claude responde de manera mas predecible en ingles.\n\n- [Leccion](#leccion)\n- [Ejercicios](#ejercicios)\n- [Area de Experimentacion](#area-de-experimentacion)\n\n## Configuracion\n\nEjecuta la siguiente celda de configuracion para cargar tu clave API y establecer la funcion auxiliar `get_completion`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install anthropic\n\n# Importar la biblioteca de expresiones regulares de Python\nimport re\nimport anthropic\n\nimport sys, os\nnotebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\nif notebook_dir not in sys.path:\n    sys.path.insert(0, notebook_dir)\n\n# Recuperar las variables API_KEY y MODEL_NAME del almacen de IPython\n%store -r API_KEY\n%store -r MODEL_NAME\n\nclient = anthropic.Anthropic(api_key=API_KEY)\n\ndef get_completion(prompt: str, system_prompt=\"\"):\n    message = client.messages.create(\n        model=MODEL_NAME,\n        max_tokens=2000,\n        temperature=0.0,\n        system=system_prompt,\n        messages=[\n          {\"role\": \"user\", \"content\": prompt}\n        ]\n    )\n    return message.content[0].text"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Leccion\n\nAnthropic ofrece dos APIs, la antigua [API de Text Completions](https://docs.anthropic.com/claude/reference/complete_post) y la actual [API de Messages](https://docs.anthropic.com/claude/reference/messages_post). Para este tutorial, usaremos exclusivamente la API de Messages.\n\nComo minimo, una llamada a Claude usando la API de Messages requiere los siguientes parametros:\n- `model`: el [nombre del modelo en la API](https://docs.anthropic.com/claude/docs/models-overview#model-recommendations) del modelo que deseas utilizar\n\n- `max_tokens`: el numero maximo de tokens a generar antes de detenerse. Ten en cuenta que Claude puede detenerse antes de alcanzar este maximo. Este parametro solo especifica el numero maximo absoluto de tokens a generar. Ademas, es una parada *forzada*, lo que significa que puede hacer que Claude deje de generar a mitad de una palabra o una oracion.\n\n- `messages`: un arreglo de mensajes de entrada. Nuestros modelos estan entrenados para operar en turnos conversacionales alternados entre `user` y `assistant`. Al crear un nuevo `Message`, especificas los turnos conversacionales previos con el parametro messages, y el modelo genera el siguiente `Message` en la conversacion.\n  - Cada mensaje de entrada debe ser un objeto con `role` y `content`. Puedes especificar un unico mensaje con rol `user`, o puedes incluir multiples mensajes de `user` y `assistant` (deben alternarse en ese caso). El primer mensaje siempre debe usar el rol `user`.\n\nTambien hay parametros opcionales, como:\n- `system`: el prompt de sistema - mas sobre esto a continuacion.\n  \n- `temperature`: el grado de variabilidad en la respuesta de Claude. Para estas lecciones y ejercicios, hemos establecido `temperature` en 0.\n\nPara una lista completa de todos los parametros de la API, visita nuestra [documentacion de la API](https://docs.anthropic.com/claude/reference/messages_post)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Ejemplos\n\nVeamos como responde Claude a algunos prompts correctamente formateados. Para cada una de las siguientes celdas, ejecuta la celda (`shift+enter`), y la respuesta de Claude aparecera debajo del bloque."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prompt\nPROMPT = \"Hi Claude, how are you?\"\n\n# Imprimir la respuesta de Claude\nprint(get_completion(PROMPT))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prompt\nPROMPT = \"Can you tell me the color of the ocean?\"\n\n# Imprimir la respuesta de Claude\nprint(get_completion(PROMPT))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prompt\nPROMPT = \"What year was Celine Dion born in?\"\n\n# Imprimir la respuesta de Claude\nprint(get_completion(PROMPT))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Ahora veamos algunos prompts que no incluyen el formato correcto de la API de Messages. Para estos prompts mal formateados, la API de Messages devuelve un error.\n\nPrimero, tenemos un ejemplo de una llamada a la API de Messages que carece de los campos `role` y `content` en el arreglo `messages`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Obtener la respuesta de Claude\nresponse = client.messages.create(\n        model=MODEL_NAME,\n        max_tokens=2000,\n        temperature=0.0,\n        messages=[\n          {\"Hi Claude, how are you?\"}\n        ]\n    )\n\n# Imprimir la respuesta de Claude\nprint(response[0].text)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Aqui hay un prompt que no alterna entre los roles `user` y `assistant`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Obtener la respuesta de Claude\nresponse = client.messages.create(\n        model=MODEL_NAME,\n        max_tokens=2000,\n        temperature=0.0,\n        messages=[\n          {\"role\": \"user\", \"content\": \"What year was Celine Dion born in?\"},\n          {\"role\": \"user\", \"content\": \"Also, can you tell me some other facts about her?\"}\n        ]\n    )\n\n# Imprimir la respuesta de Claude\nprint(response[0].text)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Los mensajes de `user` y `assistant` **DEBEN alternarse**, y los mensajes **DEBEN comenzar con un turno de `user`**. Puedes tener multiples pares de `user` y `assistant` en un prompt (como si simularas una conversacion de multiples turnos). Tambien puedes poner palabras en un mensaje final de `assistant` para que Claude continue desde donde lo dejaste (mas sobre esto en capitulos posteriores).\n\n#### Prompts de Sistema\n\nTambien puedes usar **prompts de sistema**. Un prompt de sistema es una forma de **proporcionar contexto, instrucciones y directrices a Claude** antes de presentarle una pregunta o tarea en el turno de \"User\".\n\nEstructuralmente, los prompts de sistema existen separados de la lista de mensajes de `user` y `assistant`, y por lo tanto pertenecen a un parametro `system` separado (observa la estructura de la funcion auxiliar `get_completion` en la seccion de [Configuracion](#configuracion) del notebook).\n\nDentro de este tutorial, donde podriamos utilizar un prompt de sistema, te hemos proporcionado un campo `system` en tu funcion de completions. Si no deseas usar un prompt de sistema, simplemente establece la variable `SYSTEM_PROMPT` como una cadena vacia."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Ejemplo de Prompt de Sistema"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prompt de sistema\nSYSTEM_PROMPT = \"Your answer should always be a series of critical thinking questions that further the conversation (do not provide answers to your questions). Do not actually answer the user question.\"\n\n# Prompt\nPROMPT = \"Why is the sky blue?\"\n\n# Imprimir la respuesta de Claude\nprint(get_completion(PROMPT, SYSTEM_PROMPT))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Por que usar un prompt de sistema? Un **prompt de sistema bien escrito puede mejorar el rendimiento de Claude** de diversas maneras, como aumentar la capacidad de Claude para seguir reglas e instrucciones. Para mas informacion, visita nuestra documentacion sobre [como usar prompts de sistema](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/system-prompts) con Claude.\n\nAhora nos sumergiremos en algunos ejercicios. Si deseas experimentar con los prompts de la leccion sin cambiar ningun contenido anterior, desplazate hasta el final del notebook de la leccion para visitar el [**Area de Experimentacion**](#area-de-experimentacion)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Ejercicios\n- [Ejercicio 1.1 - Contar hasta Tres](#ejercicio-11---contar-hasta-tres)\n- [Ejercicio 1.2 - Prompt de Sistema](#ejercicio-12---prompt-de-sistema)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Ejercicio 1.1 - Contar hasta Tres\nUsando el formato adecuado de `user` / `assistant`, edita el `PROMPT` a continuacion para que Claude **cuente hasta tres.** La salida tambien indicara si tu solucion es correcta."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prompt - este es el unico campo que debes cambiar\nPROMPT = \"[Replace this text]\"\n\n# Obtener la respuesta de Claude\nresponse = get_completion(PROMPT)\n\n# Funcion para calificar la correccion del ejercicio\ndef grade_exercise(text):\n    pattern = re.compile(r'^(?=.*1)(?=.*2)(?=.*3).*$', re.DOTALL)\n    return bool(pattern.match(text))\n\n# Imprimir la respuesta de Claude y la calificacion correspondiente\nprint(response)\nprint(\"\\n--------------------------- CALIFICACION ---------------------------\")\nprint(\"Este ejercicio se ha resuelto correctamente:\", grade_exercise(response))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Si quieres una pista, ejecuta la celda de abajo!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hints import exercise_1_1_hint; print(exercise_1_1_hint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Ejercicio 1.2 - Prompt de Sistema\n\nModifica el `SYSTEM_PROMPT` para que Claude responda como si fuera un nino de 3 anos."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prompt de sistema - este es el unico campo que debes cambiar\nSYSTEM_PROMPT = \"[Replace this text]\"\n\n# Prompt\nPROMPT = \"How big is the sky?\"\n\n# Obtener la respuesta de Claude\nresponse = get_completion(PROMPT, SYSTEM_PROMPT)\n\n# Funcion para calificar la correccion del ejercicio\ndef grade_exercise(text):\n    return bool(re.search(r\"giggles\", text) or re.search(r\"soo\", text))\n\n# Imprimir la respuesta de Claude y la calificacion correspondiente\nprint(response)\nprint(\"\\n--------------------------- CALIFICACION ---------------------------\")\nprint(\"Este ejercicio se ha resuelto correctamente:\", grade_exercise(response))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Si quieres una pista, ejecuta la celda de abajo!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hints import exercise_1_2_hint; print(exercise_1_2_hint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Felicidades!\n\nSi has resuelto todos los ejercicios hasta este punto, estas listo para pasar al siguiente capitulo. Feliz creacion de prompts!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Area de Experimentacion\n\nEsta es un area para que experimentes libremente con los ejemplos de prompts mostrados en esta leccion y modifiques los prompts para ver como puede afectar las respuestas de Claude."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prompt\nPROMPT = \"Hi Claude, how are you?\"\n\n# Imprimir la respuesta de Claude\nprint(get_completion(PROMPT))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prompt\nPROMPT = \"Can you tell me the color of the ocean?\"\n\n# Imprimir la respuesta de Claude\nprint(get_completion(PROMPT))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prompt\nPROMPT = \"What year was Celine Dion born in?\"\n\n# Imprimir la respuesta de Claude\nprint(get_completion(PROMPT))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Obtener la respuesta de Claude\nresponse = client.messages.create(\n        model=MODEL_NAME,\n        max_tokens=2000,\n        temperature=0.0,\n        messages=[\n          {\"Hi Claude, how are you?\"}\n        ]\n    )\n\n# Imprimir la respuesta de Claude\nprint(response[0].text)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Obtener la respuesta de Claude\nresponse = client.messages.create(\n        model=MODEL_NAME,\n        max_tokens=2000,\n        temperature=0.0,\n        messages=[\n          {\"role\": \"user\", \"content\": \"What year was Celine Dion born in?\"},\n          {\"role\": \"user\", \"content\": \"Also, can you tell me some other facts about her?\"}\n        ]\n    )\n\n# Imprimir la respuesta de Claude\nprint(response[0].text)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prompt de sistema\nSYSTEM_PROMPT = \"Your answer should always be a series of critical thinking questions that further the conversation (do not provide answers to your questions). Do not actually answer the user question.\"\n\n# Prompt\nPROMPT = \"Why is the sky blue?\"\n\n# Imprimir la respuesta de Claude\nprint(get_completion(PROMPT, SYSTEM_PROMPT))"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}